{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments results\n",
    "\n",
    "This notebook contains visualization of the results of the research. This includes the speech generated by models trained with various setups, comparison of their performance etc.\n",
    "\n",
    "This notebook assumes the following models have been trained and compiled:\n",
    "- Vanilla acoustic model without GST support\n",
    "- Acoustic model trained with GST support but compiled as vanilla\n",
    "- Acoustic model accepting GST weights as input\n",
    "- Acoustic model accepting reference speech as input\n",
    "\n",
    "\n",
    "Additionally the following models are expected to be trained;\n",
    "- GST predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First all necessary packages are imported, moreover paths to the used components should be set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data import data_loading\n",
    "from data import visualization\n",
    "from data.preprocessing import text as text_prep\n",
    "from models.gst_predictor import utils as gst_pred_utils\n",
    "from utilities import diffusion as diff_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPILED_VANILLA_PATH = '/home/devcontainer/workspace/tmp/compiled_vanilla.pt'\n",
    "COMPILED_GST_VANILLA_PATH = '/home/devcontainer/workspace/tmp/compiled_gst_vanilla.pt'\n",
    "COMPILED_GST_REF_PATH = '/home/devcontainer/workspace/tmp/compiled_gst_reference.pt'\n",
    "COMPILED_GST_WEIGHTS_PATH = '/home/devcontainer/workspace/tmp/compiled_gst_weights.pt'\n",
    "\n",
    "GST_PREDICTOR_CHECKPOINT_PATH = '/home/devcontainer/workspace/tmp/gst_predictor/checkpoints/gst_predictor_ckpt_9'\n",
    "\n",
    "LJSPEECH_DS_PATH = '/home/devcontainer/workspace/.vscode/13100-dataset'\n",
    "GST_MODEL_TB_OUTPUT_PATH = '/home/devcontainer/workspace/tmp/acoustic_gst/events.out.tfevents.1733311117.867ead6dfff8.69863.0'\n",
    "VANILLA_MODEL_TB_OUTPUT_PATH = '/home/devcontainer/workspace/tmp/acoustic_vanilla/events.out.tfevents.1733305123.867ead6dfff8.37740.0'\n",
    "MEL_TO_LIN_MODEL_TB_OUTPUT_PATH = '/home/devcontainer/workspace/tmp/mel_to_lin/events.out.tfevents.1733482633.867ead6dfff8.3499.0'\n",
    "GST_PRED_MODEL_TB_OUTPUT_PATH = '/home/devcontainer/workspace/tmp/gst_predictor/events.out.tfevents.1733681817.fd1e12a87d0d.179259.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gst_vanilla_model = torch.jit.load(COMPILED_GST_VANILLA_PATH)\n",
    "gst_ref_model = torch.jit.load(COMPILED_GST_REF_PATH)\n",
    "gst_weights_model = torch.jit.load(COMPILED_GST_WEIGHTS_PATH)\n",
    "\n",
    "gst_tb_events = event_accumulator.EventAccumulator(GST_MODEL_TB_OUTPUT_PATH)\n",
    "gst_tb_events.Reload()\n",
    "\n",
    "vanilla_tb_events = event_accumulator.EventAccumulator(VANILLA_MODEL_TB_OUTPUT_PATH)\n",
    "vanilla_tb_events.Reload()\n",
    "\n",
    "mel_to_lin_tb_events = event_accumulator.EventAccumulator(MEL_TO_LIN_MODEL_TB_OUTPUT_PATH)\n",
    "mel_to_lin_tb_events.Reload()\n",
    "\n",
    "gst_pred_tb_events = event_accumulator.EventAccumulator(GST_PRED_MODEL_TB_OUTPUT_PATH)\n",
    "gst_pred_tb_events.Reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of models' results\n",
    "\n",
    "In this chapter compiled models are going to be loaded and, depending on the particular model's setup, speech samples in different configurations shall be generated and visualized.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_spectrogram_with_phoneme_durations(spectrogram, durations, title):\n",
    "\n",
    "    frame_boundaries = np.cumsum(durations)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "    ax.imshow(spectrogram, aspect='auto', origin='lower', cmap='viridis')\n",
    "\n",
    "    for i, _ in enumerate(durations):\n",
    "\n",
    "        if i > 0:\n",
    "            ax.axvline(frame_boundaries[i - 1], color='red', linestyle='--')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Time bin index')\n",
    "    ax.set_ylabel('Frequency bin index')\n",
    "\n",
    "_, _, test_ds = data_loading.get_datasets(os.path.join(LJSPEECH_DS_PATH, 'split'),\n",
    "                                          train_split_ratio=0.98,\n",
    "                                          n_test_files=100)\n",
    "\n",
    "sample_spectrogram, sample_transcript, sample_log_durations = test_ds[0]\n",
    "\n",
    "print('Example input phonemes:', visualization.decode_transcript(\n",
    "    sample_transcript, text_prep.ENHANCED_MFA_ARP_VOCAB\n",
    "))\n",
    "\n",
    "durations_mask = (sample_log_durations.numpy() > 0).astype(np.uint16)\n",
    "pow_durations = (np.power(2, sample_log_durations.numpy()) +\n",
    "                    1e-4).astype(np.uint16)[:np.sum(durations_mask).item()]\n",
    "\n",
    "visualization.annotate_spectrogram_with_phoneme_durations(\n",
    "    sample_spectrogram, pow_durations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vanilla_output(inference_cfg,\n",
    "                             cfg_name,\n",
    "                             gt_wav_path):\n",
    "    \n",
    "    with open(f'.experiments_results/{cfg_name}.json', 'w') as f:\n",
    "        json.dump(inference_cfg, f)\n",
    "\n",
    "    !PYTHONPATH=../src python ../scripts/inference/run_inference.py --config_path .experiments_results/{cfg_name}.json\n",
    "\n",
    "    spec_trans = transforms.Compose([\n",
    "        torchaudio.transforms.Spectrogram(n_fft=1024, hop_length=256, win_length=256),\n",
    "        torchaudio.transforms.AmplitudeToDB()\n",
    "    ])\n",
    "\n",
    "    gt_waveform, _ = torchaudio.load(gt_wav_path)\n",
    "    output_waveform, _ = torchaudio.load(inference_cfg['output_path'])\n",
    "\n",
    "    gt_spec = spec_trans(gt_waveform)\n",
    "    output_spec = spec_trans(output_waveform)\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 1, figsize=(20, 20))\n",
    "    ax[0].imshow(gt_spec[0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    ax[0].set_title('Ground truth spectrogram')\n",
    "\n",
    "    ax[1].imshow(output_spec[0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    ax[1].set_title('Output spectrogram')\n",
    "\n",
    "def visualize_gst_weights_output(inference_cfg,\n",
    "                                 cfg_name,\n",
    "                                 gt_wav_path):\n",
    "    \n",
    "    visualize_vanilla_output(inference_cfg, cfg_name, gt_wav_path)\n",
    "\n",
    "    gst_weights = torch.load(inference_cfg['gst_weights_cfg']['weights_path'], weights_only=True)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n",
    "\n",
    "    ax.plot(gst_weights)\n",
    "    ax.set_title('GST weights')\n",
    "\n",
    "\n",
    "def visualize_gst_reference_output(inference_cfg,\n",
    "                                   cfg_name,\n",
    "                                   gt_wav_path):\n",
    "    \n",
    "    visualize_vanilla_output(inference_cfg, cfg_name, gt_wav_path)\n",
    "\n",
    "    gst_ref_waveform, _ = torchaudio.load(inference_cfg['gst_reference_cfg']['reference_audio_path'])\n",
    "\n",
    "    gst_ref_spec = transforms.Compose([\n",
    "        torchaudio.transforms.Spectrogram(n_fft=1024, hop_length=256, win_length=256),\n",
    "        torchaudio.transforms.AmplitudeToDB()\n",
    "    ])(gst_ref_waveform)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    ax.imshow(gst_ref_spec[0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    ax.set_title('GST reference spectrogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_vanilla_output(\n",
    "    {\n",
    "        \"compiled_model_path\": COMPILED_VANILLA_PATH,\n",
    "        \"input_phonemes_length\": 20,\n",
    "        \"input_text\": \"The crime, long carried on without detection, was first discovered in eighteen twenty.\",\n",
    "        \"gst_mode\": \"none\",\n",
    "        \"scale_max\": 45.8506,\n",
    "        \"scale_min\": -100.0,\n",
    "        \"output_path\": \".experiments_results/vanilla_output.wav\",\n",
    "    },\n",
    "    'vanilla_inference_cfg',\n",
    "    os.path.join(LJSPEECH_DS_PATH, 'raw/LJSpeech-1.1/wavs/LJ011-0018.wav')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla GST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_vanilla_output(\n",
    "    {\n",
    "        \"compiled_model_path\": COMPILED_GST_VANILLA_PATH,\n",
    "        \"input_phonemes_length\": 20,\n",
    "        \"input_text\": \"The crime, long carried on without detection, was first discovered in eighteen twenty.\",\n",
    "        \"gst_mode\": \"none\",\n",
    "        \"scale_max\": 45.8506,\n",
    "        \"scale_min\": -100.0,\n",
    "        \"output_path\": \".experiments_results/vanilla_gst_output.wav\",\n",
    "    },\n",
    "    'vanilla_gst_inference_cfg',\n",
    "    os.path.join(LJSPEECH_DS_PATH, 'raw/LJSpeech-1.1/wavs/LJ011-0018.wav')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GST model with input weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = torch.tensor([0.0003, 0.0100, 0.0038, 0.1589, 0.0324, 0.0657, 0.0221, 0.0317, 0.0155,\n",
    "        0.0051, 0.0652, 0.0430, 0.0019, 0.0403, 0.0196, 0.0174, 0.0654, 0.0011,\n",
    "        0.0280, 0.0002, 0.0115, 0.0107, 0.0223, 0.0610, 0.0195, 0.0172, 0.0160,\n",
    "        0.0040, 0.0269, 0.0028, 0.0345, 0.0111])\n",
    "\n",
    "torch.save(weights, '.experiments_results/gst_weights_1.pt')\n",
    "\n",
    "visualize_gst_weights_output(\n",
    "    {\n",
    "        \"compiled_model_path\": COMPILED_GST_WEIGHTS_PATH,\n",
    "        \"input_phonemes_length\": 20,\n",
    "        \"input_text\": \"The crime, long carried on without detection, was first discovered in eighteen twenty.\",\n",
    "        \"gst_mode\": \"weights\",\n",
    "        \"scale_max\": 45.8506,\n",
    "        \"scale_min\": -100.0,\n",
    "        \"output_path\": \".experiments_results/gst_weights_output_1.wav\",\n",
    "        \"gst_weights_cfg\": {\n",
    "            \"weights_path\": \".experiments_results/gst_weights_1.pt\"\n",
    "        }\n",
    "    },\n",
    "    'gst_weights_inference_cfg_1',\n",
    "    os.path.join(LJSPEECH_DS_PATH, 'raw/LJSpeech-1.1/wavs/LJ011-0018.wav')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.0095, 0.0242, 0.0075, 0.1424, 0.0184, 0.0415, 0.0071, 0.0085, 0.0049,\n",
    "        0.0405, 0.0976, 0.0585, 0.1090, 0.0401, 0.0091, 0.0186, 0.0436, 0.0144,\n",
    "        0.0827, 0.0013, 0.0338, 0.0018, 0.0119, 0.0194, 0.0384, 0.0081, 0.0124,\n",
    "        0.0512, 0.0120, 0.0130, 0.0157, 0.0028])\n",
    "\n",
    "torch.save(weights, '.experiments_results/gst_weights_2.pt')\n",
    "\n",
    "visualize_gst_weights_output(\n",
    "    {\n",
    "        \"compiled_model_path\": COMPILED_GST_WEIGHTS_PATH,\n",
    "        \"input_phonemes_length\": 20,\n",
    "        \"input_text\": \"The crime, long carried on without detection, was first discovered in eighteen twenty.\",\n",
    "        \"gst_mode\": \"weights\",\n",
    "        \"scale_max\": 45.8506,\n",
    "        \"scale_min\": -100.0,\n",
    "        \"output_path\": \".experiments_results/gst_weights_output_2.wav\",\n",
    "        \"gst_weights_cfg\": {\n",
    "            \"weights_path\": \".experiments_results/gst_weights_2.pt\"\n",
    "        }\n",
    "    },\n",
    "    'gst_weights_inference_cfg_2',\n",
    "    os.path.join(LJSPEECH_DS_PATH, 'raw/LJSpeech-1.1/wavs/LJ011-0018.wav')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GST model with input reference speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gst_reference_output(\n",
    "    {\n",
    "        \"compiled_model_path\": COMPILED_GST_REF_PATH,\n",
    "        \"input_phonemes_length\": 20,\n",
    "        \"input_text\": \"The crime, long carried on without detection, was first discovered in eighteen twenty.\",\n",
    "        \"gst_mode\": \"reference\",\n",
    "        \"scale_max\": 45.8506,\n",
    "        \"scale_min\": -100.0,\n",
    "        \"output_path\": \".experiments_results/gst_reference_output.wav\",\n",
    "        \"gst_reference_cfg\": {\n",
    "            \"reference_audio_path\": os.path.join(LJSPEECH_DS_PATH, 'raw/LJSpeech-1.1/wavs/LJ011-0018.wav'),\n",
    "            \"spectrogram_window_length\": 1024,\n",
    "            \"spectrogram_hop_length\": 256,\n",
    "            \"n_mels\": 80,\n",
    "            \"spec_length\": 200,\n",
    "            \"sample_rate\": 22050\n",
    "        },\n",
    "    },\n",
    "    'gst_reference_inference_cfg',\n",
    "    os.path.join(LJSPEECH_DS_PATH, 'raw/LJSpeech-1.1/wavs/LJ011-0018.wav')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GST Model with predicted GST weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSTPredictorInferenceModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder: torch.nn.Module,\n",
    "                 decoder: torch.nn.Module,\n",
    "                 global_mean: torch.Tensor,\n",
    "                 global_std: torch.Tensor,\n",
    "                 diff_handler: diff_utils.DiffusionHandler,\n",
    "                 n_weights: int):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self._encoder = encoder\n",
    "        self._decoder = decoder\n",
    "        self._global_mean = global_mean\n",
    "        self._global_std = global_std\n",
    "        self._diff_handler = diff_handler\n",
    "        self._n_weights = n_weights\n",
    "\n",
    "    def forward(self, phonemes):\n",
    "\n",
    "        noised_data = torch.randn(1, self._n_weights)\n",
    "        phoneme_embedding = self._encoder(phonemes)\n",
    "\n",
    "        for timestep in reversed(range(self._diff_handler.num_steps)):\n",
    "\n",
    "            predicted_noise = self._decoder(noised_data,\n",
    "                                            torch.Tensor([timestep]),\n",
    "                                            phoneme_embedding)\n",
    "\n",
    "            noised_data = self._diff_handler.remove_noise(noised_data,\n",
    "                                                          predicted_noise,\n",
    "                                                          timestep)\n",
    "\n",
    "        return (noised_data * self._global_std) + self._global_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gst_pred_cfg = {\n",
    "    \"decoder\": {\n",
    "        \"timestep_embedding_size\": 256,\n",
    "        \"internal_channels\": 64,\n",
    "        \"n_conv_blocks\": 6\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"n_conv_blocks\": 6,\n",
    "        \"embedding_size\": 128\n",
    "    },\n",
    "    \"dropout_rate\": 0.2\n",
    "}\n",
    "\n",
    "diff_cfg = {\n",
    "    \"n_steps\": 1000,\n",
    "    \"beta_min\": 0.0001,\n",
    "    \"beta_max\": 0.02\n",
    "}\n",
    "\n",
    "global_mean = torch.tensor(\n",
    "            [0.0362, 0.0354, 0.0347, 0.0343, 0.0259, 0.0346, 0.0244, 0.0342, 0.0281,\n",
    "             0.0316, 0.0365, 0.0270, 0.0412, 0.0305, 0.0279, 0.0336, 0.0336, 0.0341,\n",
    "             0.0292, 0.0277, 0.0288, 0.0273, 0.0328, 0.0329, 0.0305, 0.0232, 0.0295,\n",
    "             0.0271, 0.0316, 0.0271, 0.0413, 0.0273])\n",
    "\n",
    "global_stddev = torch.tensor(\n",
    "        [0.0289, 0.0285, 0.0275, 0.0289, 0.0193, 0.0269, 0.0212, 0.0265, 0.0265,\n",
    "        0.0249, 0.0389, 0.0205, 0.0345, 0.0271, 0.0283, 0.0286, 0.0286, 0.0281,\n",
    "        0.0233, 0.0227, 0.0224, 0.0250, 0.0324, 0.0276, 0.0263, 0.0215, 0.0259,\n",
    "        0.0234, 0.0255, 0.0227, 0.0366, 0.0225])\n",
    "\n",
    "gst_pred_components = gst_pred_utils.create_model_components((20, 73), gst_pred_cfg, 'cpu')\n",
    "\n",
    "gst_predictor_inf = GSTPredictorInferenceModel(\n",
    "    gst_pred_components.encoder,\n",
    "    gst_pred_components.decoder,\n",
    "    global_mean,\n",
    "    global_stddev,\n",
    "    diff_utils.DiffusionHandler(\n",
    "        diff_utils.LinearScheduler(diff_cfg['beta_min'], diff_cfg['beta_max'], diff_cfg['n_steps']),\n",
    "        'cpu'\n",
    "    ),\n",
    "    32\n",
    ")\n",
    "\n",
    "gst_predictor_inf = gst_predictor_inf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'The crime, long carried on without detection, was first discovered in eighteen twenty.'\n",
    "phonemes = text_prep.G2PTransform()(input_text)\n",
    "encoded_phonemes = transforms.Compose([\n",
    "    text_prep.PadSequenceTransform(20),\n",
    "    text_prep.OneHotEncodeTransform(text_prep.ENHANCED_MFA_ARP_VOCAB)\n",
    "])(phonemes)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_gst = gst_predictor_inf(encoded_phonemes.unsqueeze(0))\n",
    "    predicted_gst = torch.functional.F.softmax(predicted_gst, dim=-1)\n",
    "    torch.save(predicted_gst.squeeze(0), '.experiments_results/gst_predicted_weights.pt')\n",
    "\n",
    "visualize_gst_weights_output(\n",
    "    {\n",
    "        \"compiled_model_path\": COMPILED_GST_WEIGHTS_PATH,\n",
    "        \"input_phonemes_length\": 20,\n",
    "        \"input_text\": \"The crime, long carried on without detection, was first discovered in eighteen twenty.\",\n",
    "        \"gst_mode\": \"weights\",\n",
    "        \"scale_max\": 45.8506,\n",
    "        \"scale_min\": -100.0,\n",
    "        \"output_path\": \".experiments_results/gst_predicted_weights_output.wav\",\n",
    "        \"gst_weights_cfg\": {\n",
    "            \"weights_path\": \".experiments_results/gst_predicted_weights.pt\"\n",
    "        }\n",
    "    },\n",
    "    'gst_predicted_weights_inference_cfg',\n",
    "    os.path.join(LJSPEECH_DS_PATH, 'raw/LJSpeech-1.1/wavs/LJ011-0018.wav')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
